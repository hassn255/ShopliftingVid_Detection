{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"GPU name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Nlj6kKBg8ut",
        "outputId": "b248405c-2982-4387-e971-414b186e8bfc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "GPU name: CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9NrWq_dXtpp",
        "outputId": "aeb64955-cee4-411d-a41b-ec778380a3aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 1: Dataset Extraction"
      ],
      "metadata": {
        "id": "nfiSecHCfqxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple one-liner extraction (if you just want to quickly extract)\n",
        "import zipfile\n",
        "zip_path = \"/content/drive/MyDrive/project/Shop DataSet.zip\"\n",
        "extract_path = \"/content/drive/MyDrive/project\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "    print(f\"Extracted to {extract_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrM0WA1ifqR9",
        "outputId": "661e9ad0-f6f9-4967-ddf8-7ff6fce8f8c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted to /content/drive/MyDrive/project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset path\n",
        "dataset_path = '/content/drive/MyDrive/project/Shop DataSet'\n",
        "\n",
        "# Check dataset structure\n",
        "def explore_dataset(path):\n",
        "    print(\"Dataset structure:\")\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        level = root.replace(path, '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files[:5]:  # Show first 5 files\n",
        "            print(f\"{subindent}{file}\")\n",
        "        if len(files) > 5:\n",
        "            print(f\"{subindent}... and {len(files) - 5} more files\")\n",
        "\n",
        "explore_dataset(dataset_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nuUVuq7kBPT",
        "outputId": "be7245bc-280d-4b7d-8a4a-c56635886fea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset structure:\n",
            "Shop DataSet/\n",
            "  non shop lifters/\n",
            "    shop_lifter_n_0.mp4\n",
            "    shop_lifter_n_0_1.mp4\n",
            "    shop_lifter_n_1.mp4\n",
            "    shop_lifter_n_1_1.mp4\n",
            "    shop_lifter_n_10.mp4\n",
            "    ... and 526 more files\n",
            "  shop lifters/\n",
            "    shop_lifter_0.mp4\n",
            "    shop_lifter_1.mp4\n",
            "    shop_lifter_10.mp4\n",
            "    shop_lifter_100.mp4\n",
            "    shop_lifter_101.mp4\n",
            "    ... and 319 more files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 2: Dataset Class and Data Loading"
      ],
      "metadata": {
        "id": "-kaOxW3qaT7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, video_paths, labels, transform=None, sequence_length=16, frame_size=(112, 112)):\n",
        "        self.video_paths = video_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.sequence_length = sequence_length\n",
        "        self.frame_size = frame_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Read video frames\n",
        "        frames = self.load_video_frames(video_path)\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            frames = torch.stack([self.transform(frame) for frame in frames])\n",
        "        else:\n",
        "            frames = torch.stack([transforms.ToTensor()(frame) for frame in frames])\n",
        "\n",
        "        return frames, label\n",
        "\n",
        "    def load_video_frames(self, video_path):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        frame_interval = max(total_frames // self.sequence_length, 1)\n",
        "\n",
        "        frame_count = 0\n",
        "        while len(frames) < self.sequence_length:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_count % frame_interval == 0:\n",
        "                # Resize frame\n",
        "                frame = cv2.resize(frame, self.frame_size)\n",
        "                # Convert BGR to RGB\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frames.append(frame)\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # If we don't have enough frames, duplicate the last frame\n",
        "        while len(frames) < self.sequence_length:\n",
        "            frames.append(frames[-1])\n",
        "\n",
        "        return frames[:self.sequence_length]\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "u1O91oVfaSnf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Load and prepare dataset"
      ],
      "metadata": {
        "id": "xzoipQcfkPFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(dataset_path):\n",
        "    video_paths = []\n",
        "    labels = []\n",
        "\n",
        "    # Define class folders\n",
        "    class_folders = ['non shop lifters', 'shop lifters']\n",
        "    class_mapping = {'non shop lifters': 0, 'shop lifters': 1}\n",
        "\n",
        "    for class_folder in class_folders:\n",
        "        class_path = os.path.join(dataset_path, class_folder)\n",
        "        if os.path.exists(class_path):\n",
        "            for video_file in os.listdir(class_path):\n",
        "                if video_file.endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
        "                    video_paths.append(os.path.join(class_path, video_file))\n",
        "                    labels.append(class_mapping[class_folder])\n",
        "\n",
        "    return video_paths, labels\n",
        "\n",
        "# Load all video paths and labels\n",
        "video_paths, labels = load_dataset(dataset_path)\n",
        "\n",
        "print(f\"Total videos: {len(video_paths)}\")\n",
        "print(f\"Class distribution: {np.unique(labels, return_counts=True)}\")\n",
        "\n",
        "# Split dataset into train and validation\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    video_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_paths)}\")\n",
        "print(f\"Validation samples: {len(val_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hly-vB9-kM7_",
        "outputId": "e7d741a1-e4b6-4228-e3a5-54be977e30df"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos: 855\n",
            "Class distribution: (array([0, 1]), array([531, 324]))\n",
            "Training samples: 684\n",
            "Validation samples: 171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Create data loaders"
      ],
      "metadata": {
        "id": "fPgF4IhzkU9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = VideoDataset(train_paths, train_labels, transform=transform, sequence_length=16)\n",
        "val_dataset = VideoDataset(val_paths, val_labels, transform=transform, sequence_length=16)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 8\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Test one batch\n",
        "for frames, labels in train_loader:\n",
        "    print(f\"Batch frames shape: {frames.shape}\")  # Should be [batch_size, sequence_length, channels, height, width]\n",
        "    print(f\"Batch labels shape: {labels.shape}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwmMPBPmkXYA",
        "outputId": "45e10962-d801-4974-ebdb-d90d4b66933e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch frames shape: torch.Size([8, 16, 3, 112, 112])\n",
            "Batch labels shape: torch.Size([8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 3: 3DCNN Model Architecture"
      ],
      "metadata": {
        "id": "GOIrLEs-cOEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Simple3DCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(Simple3DCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.bn1 = nn.BatchNorm3d(64)\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
        "\n",
        "        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.bn2 = nn.BatchNorm3d(128)\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
        "\n",
        "        self.conv3 = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.bn3 = nn.BatchNorm3d(256)\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
        "\n",
        "        self.conv4 = nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
        "        self.bn4 = nn.BatchNorm3d(512)\n",
        "        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: [batch_size, channels, depth, height, width]\n",
        "        x = x.permute(0, 2, 1, 3, 4)  # Change from [B, D, C, H, W] to [B, C, D, H, W]\n",
        "\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = torch.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.pool4(x)\n",
        "\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "model = Simple3DCNN(num_classes=2).to(device)\n",
        "print(model)\n",
        "\n",
        "# Test model with sample input\n",
        "sample_input = torch.randn(2, 16, 3, 112, 112).to(device)  # [batch, sequence, channels, height, width]\n",
        "sample_output = model(sample_input)\n",
        "print(f\"Sample output shape: {sample_output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m29AqVeycKD5",
        "outputId": "5a57cd20-2859-4927-d1ed-08dee2b16751"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple3DCNN(\n",
            "  (conv1): Conv3d(3, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool1): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool2): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv4): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (bn4): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (global_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n",
            "Sample output shape: torch.Size([2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define 3DCNN model\n"
      ],
      "metadata": {
        "id": "gTw9Im7vfI27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_data_loaders(data_dir, batch_size=8, frames_per_clip=16, img_size=112, use_opencv=True):\n",
        "    # Get all video paths and labels\n",
        "    video_paths, labels = get_video_paths_and_labels(data_dir)\n",
        "\n",
        "    print(f\"Total videos found: {len(video_paths)}\")\n",
        "    print(f\"Class distribution: {np.bincount(labels)}\")\n",
        "\n",
        "    # Split data into training and validation\n",
        "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "        video_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    print(f\"Training samples: {len(train_paths)}\")\n",
        "    print(f\"Validation samples: {len(val_paths)}\")\n",
        "\n",
        "    # Define transformations for PIL Images\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = VideoDataset(train_paths, train_labels, train_transform, frames_per_clip, use_opencv=use_opencv)\n",
        "    val_dataset = VideoDataset(val_paths, val_labels, val_transform, frames_per_clip, use_opencv=use_opencv)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "61OUmcLod760"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define training setup"
      ],
      "metadata": {
        "id": "z-qNaTiZfMfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc='Training')\n",
        "    for batch_idx, (frames, labels) in enumerate(pbar):\n",
        "        frames = frames.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(frames)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
        "            'Acc': f'{100.*correct/total:.2f}%'\n",
        "        })\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Validation function\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(dataloader, desc='Validation')\n",
        "        for batch_idx, (frames, labels) in enumerate(pbar):\n",
        "            frames = frames.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(frames)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
        "                'Acc': f'{100.*correct/total:.2f}%'\n",
        "            })\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "2Vmn-uB2fNQI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "zMKorhxVkzxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "num_epochs = 20\n",
        "best_val_acc = 0.0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "    print('-' * 50)\n",
        "\n",
        "    # Training phase\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Validation phase\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'best_3dcnn_model.pth')\n",
        "        print(f'New best model saved with validation accuracy: {val_acc:.2f}%')\n",
        "\n",
        "print(f'\\nTraining completed. Best validation accuracy: {best_val_acc:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xjNVeD8ck0cA",
        "outputId": "2a57c661-4769-4287-80a7-d550473c9668"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "\n",
            "Epoch 1/20\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 86/86 [09:50<00:00,  6.86s/it, Loss=0.4829, Acc=78.36%]\n",
            "Validation: 100%|██████████| 22/22 [00:57<00:00,  2.62s/it, Loss=5.0423, Acc=61.99%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4829, Train Acc: 78.36%\n",
            "Val Loss: 5.0423, Val Acc: 61.99%\n",
            "New best model saved with validation accuracy: 61.99%\n",
            "\n",
            "Epoch 2/20\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 86/86 [09:45<00:00,  6.80s/it, Loss=0.1441, Acc=94.44%]\n",
            "Validation: 100%|██████████| 22/22 [00:57<00:00,  2.60s/it, Loss=12.5060, Acc=38.01%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1441, Train Acc: 94.44%\n",
            "Val Loss: 12.5060, Val Acc: 38.01%\n",
            "\n",
            "Epoch 3/20\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 86/86 [09:42<00:00,  6.77s/it, Loss=0.0566, Acc=98.39%]\n",
            "Validation: 100%|██████████| 22/22 [00:57<00:00,  2.62s/it, Loss=6.1658, Acc=38.01%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0566, Train Acc: 98.39%\n",
            "Val Loss: 6.1658, Val Acc: 38.01%\n",
            "\n",
            "Epoch 4/20\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 86/86 [09:41<00:00,  6.76s/it, Loss=0.0686, Acc=98.25%]\n",
            "Validation: 100%|██████████| 22/22 [00:58<00:00,  2.65s/it, Loss=17.3077, Acc=38.01%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0686, Train Acc: 98.25%\n",
            "Val Loss: 17.3077, Val Acc: 38.01%\n",
            "\n",
            "Epoch 5/20\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 86/86 [09:43<00:00,  6.78s/it, Loss=0.0504, Acc=98.68%]\n",
            "Validation: 100%|██████████| 22/22 [00:57<00:00,  2.61s/it, Loss=0.1517, Acc=95.91%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0504, Train Acc: 98.68%\n",
            "Val Loss: 0.1517, Val Acc: 95.91%\n",
            "New best model saved with validation accuracy: 95.91%\n",
            "\n",
            "Epoch 6/20\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 86/86 [09:43<00:00,  6.78s/it, Loss=0.0377, Acc=99.12%]\n",
            "Validation: 100%|██████████| 22/22 [00:59<00:00,  2.72s/it, Loss=0.0014, Acc=100.00%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0377, Train Acc: 99.12%\n",
            "Val Loss: 0.0014, Val Acc: 100.00%\n",
            "New best model saved with validation accuracy: 100.00%\n",
            "\n",
            "Epoch 7/20\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  94%|█████████▍| 81/86 [09:23<00:34,  6.96s/it, Loss=0.0308, Acc=99.23%]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1401014076.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Validation phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1139997990.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot training results"
      ],
      "metadata": {
        "id": "daxjtn8sk3so"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label='Training Accuracy')\n",
        "plt.plot(val_accs, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q-qRvOUJk4mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the final model"
      ],
      "metadata": {
        "id": "9D-kDUzfk72g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final model\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses,\n",
        "    'train_accs': train_accs,\n",
        "    'val_accs': val_accs,\n",
        "    'best_val_acc': best_val_acc\n",
        "}, 'final_3dcnn_model.pth')\n",
        "\n",
        "print(\"Model saved successfully!\")\n",
        "\n",
        "# Save to Google Drive\n",
        "import shutil\n",
        "shutil.copy('final_3dcnn_model.pth', '/content/drive/MyDrive/project/final_3dcnn_model.pth')\n",
        "shutil.copy('best_3dcnn_model.pth', '/content/drive/MyDrive/project/best_3dcnn_model.pth')\n",
        "print(\"Models copied to Google Drive!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "IECOrwOuk831",
        "outputId": "228d1986-a3fb-44e0-a5bf-3979d747d72d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-580965167.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the final model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m torch.save({\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'train_losses'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    }
  ]
}